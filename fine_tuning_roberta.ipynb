{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1A8MhXtJTGi"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqZP7THdu4MS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip show transformers\n",
        "!pip show accelerate\n",
        "!pip install transformers[torch] -U\n",
        "!pip install accelerate -U\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip install --upgrade transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "seFYyv2UsjtM",
        "outputId": "e9ec9fe6-6703-4d5b-d165-530ed26c3968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.37.2\n",
            "Uninstalling transformers-4.37.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.37.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled transformers-4.37.2\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.37.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERNlGyweKUhm"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfScIJapKW5X",
        "outputId": "f6d82d86-f912-4520-e526-ca7233fd076b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-3b2408560d37>:21: DtypeWarning: Columns (17,18,19,20,21,22,23,24,25,26,27,28,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  toxicity_train_df = pd.read_csv('toxicity_train.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
            "<ipython-input-35-3b2408560d37>:26: DtypeWarning: Columns (17,18,19,20,21,22,23,24,25,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  toxicity_test_df = pd.read_csv('toxicity_test.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution in training data before undersampling:\n",
            "0.0    1533\n",
            "1.0     381\n",
            "Name: toxic, dtype: int64\n",
            "\n",
            "Class distribution in test data before undersampling:\n",
            "0.0    5617\n",
            "1.0    1670\n",
            "Name: toxic, dtype: int64\n",
            "\n",
            "Class distribution in training data after undersampling:\n",
            "0.0    766\n",
            "1.0    381\n",
            "Name: toxic, dtype: int64\n",
            "\n",
            "Class distribution in test data after undersampling:\n",
            "0.0    5617\n",
            "1.0    1670\n",
            "Name: toxic, dtype: int64\n",
            "Toxic train examples\n",
            "                                        comment_text  toxic   obscene  \\\n",
            "0                                        didn't he?\"    0.0  0.000000   \n",
            "1   white supremacists and all other left wing gr...    1.0  0.000000   \n",
            "2   this man deserves the chair. He ruined the li...    1.0  0.008264   \n",
            "3   and casts his votes based on fact based commo...    0.0  0.000000   \n",
            "\n",
            "   sexual_explicit    threat    insult  identity_attack  \n",
            "0              0.0  0.000000  0.000000         0.000000  \n",
            "1              0.0  0.000000  0.300000         0.300000  \n",
            "2              0.0  0.845518  0.086459         0.007629  \n",
            "3              0.0  0.000000  0.000000         0.000000  \n",
            "Toxic test examples\n",
            "                                          comment_text  toxic  obscene  \\\n",
            "24                                                BS !    0.0      0.0   \n",
            "56   Hodad...does your user handle refer to your da...    0.0      0.0   \n",
            "156  Many of them were wearing red Na'i Aupuni shir...    0.0      0.0   \n",
            "181  \"Methinks your comment is indicative of your n...    0.0      0.0   \n",
            "\n",
            "     sexual_explicit    threat  insult  identity_attack  \n",
            "24               0.0  0.000000     0.0              0.0  \n",
            "56               0.0  0.000000     0.0              0.0  \n",
            "156              0.0  0.000000     0.0              0.0  \n",
            "181              0.0  0.166667     0.0              0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-3b2408560d37>:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  toxicity_test_df[categories_to_check] = toxicity_test_df[categories_to_check].apply(pd.to_numeric, errors='coerce')\n",
            "<ipython-input-35-3b2408560d37>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  toxicity_test_df['toxic'] = (toxicity_test_df[categories_to_check] >= 0.25).any(axis=1).astype(float)\n",
            "<ipython-input-35-3b2408560d37>:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  toxicity_test_df['toxic'] = toxicity_test_df['toxic'].astype(float)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "# 1. Prepare Dataset\n",
        "# 2. Load pretrained Tokenizer, call it with dataset -> encoding\n",
        "# 3. Build PyTorth Dataset with encodings\n",
        "# 4. Load pretrained Model\n",
        "# 5. Load HF Trainer and train it\n",
        "\n",
        "\n",
        "# Train Data - 52662 rows after pre-processing (done above)\n",
        "toxicity_train_df = pd.read_csv('toxicity_train.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
        "toxicity_train_df = toxicity_train_df.dropna()\n",
        "\n",
        "\n",
        "# Test Data - 7287 rows after pre-processing\n",
        "toxicity_test_df = pd.read_csv('toxicity_test.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)\n",
        "toxicity_test_df = toxicity_test_df.dropna()\n",
        "\n",
        "\n",
        "# Sampling for CPU use\n",
        "# fraction = 0.1\n",
        "\n",
        "# toxicity_train_df = toxicity_train_df.sample(frac=fraction, random_state=42)\n",
        "# toxicity_test_df = toxicity_test_df.sample(frac=fraction, random_state=42)\n",
        "\n",
        "# # Reset index for consistency\n",
        "# toxicity_train_df.reset_index(drop=True, inplace=True)\n",
        "# toxicity_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# List of categories to check\n",
        "categories_to_check = ['obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']\n",
        "\n",
        "# Check if any category is above the 0.5 threshold\n",
        "toxicity_train_df[categories_to_check] = toxicity_train_df[categories_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "toxicity_test_df[categories_to_check] = toxicity_test_df[categories_to_check].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check if any category is above the 0.5 threshold\n",
        "toxicity_train_df['toxic'] = (toxicity_train_df[categories_to_check] >= 0.25).any(axis=1).astype(float)\n",
        "toxicity_test_df['toxic'] = (toxicity_test_df[categories_to_check] >= 0.25).any(axis=1).astype(float)\n",
        "\n",
        "# Convert boolean values to 1.0 for True and 0.0 for False\n",
        "toxicity_train_df['toxic'] = toxicity_train_df['toxic'].astype(float)\n",
        "toxicity_test_df['toxic'] = toxicity_test_df['toxic'].astype(float)\n",
        "\n",
        "toxicity_train_df = toxicity_train_df[['comment_text', 'toxic', 'obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']]\n",
        "toxicity_test_df = toxicity_test_df[['comment_text', 'toxic', 'obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']]\n",
        "\n",
        "print(\"\\nClass distribution in training data before undersampling:\")\n",
        "print(toxicity_train_df['toxic'].value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in test data before undersampling:\")\n",
        "print(toxicity_test_df['toxic'].value_counts())\n",
        "\n",
        "\n",
        "# Fraction of the majority class you want to keep (e.g., 0.2 for 20%)\n",
        "undersample_fraction = 0.5\n",
        "\n",
        "# Separate the majority and minority classes in training data\n",
        "majority_class_train = toxicity_train_df[toxicity_train_df['toxic'] == 0]\n",
        "minority_class_train = toxicity_train_df[toxicity_train_df['toxic'] == 1]\n",
        "\n",
        "# Undersample the majority class in training data\n",
        "undersampled_majority_class_train = majority_class_train.sample(frac=undersample_fraction, random_state=42)\n",
        "\n",
        "# Concatenate the undersampled majority class with the minority class in training data\n",
        "undersampled_train_df = pd.concat([undersampled_majority_class_train, minority_class_train])\n",
        "\n",
        "# Shuffle the undersampled training dataframe\n",
        "toxicity_train_df = undersampled_train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Separate the majority and minority classes in test data\n",
        "majority_class_test = toxicity_test_df[toxicity_test_df['toxic'] == 0]\n",
        "minority_class_test = toxicity_test_df[toxicity_test_df['toxic'] == 1]\n",
        "\n",
        "# Display the class distribution after undersampling\n",
        "print(\"\\nClass distribution in training data after undersampling:\")\n",
        "print(toxicity_train_df['toxic'].value_counts())\n",
        "\n",
        "print(\"\\nClass distribution in test data after undersampling:\")\n",
        "print(toxicity_test_df['toxic'].value_counts())\n",
        "\n",
        "print(\"Toxic train examples\")\n",
        "print(toxicity_train_df.head(4))\n",
        "\n",
        "print(\"Toxic test examples\")\n",
        "print(toxicity_test_df.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckNEOfOt7JY0"
      },
      "source": [
        "Test Lengths of DFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEr4-GKc7LG4",
        "outputId": "8e6e27f8-b6f3-475b-cd34-dbe4ff963622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1147\n",
            "7287\n"
          ]
        }
      ],
      "source": [
        "print(len(toxicity_train_df))\n",
        "print(len(toxicity_test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCaSSAkv-L2D"
      },
      "source": [
        "# Visualization of toxicity in train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "Wh3Wr23h-Nwl",
        "outputId": "c99df260-9e70-4007-cc84-e61a43a750b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Count toxic and non-toxic comments\n",
        "toxic_count = toxicity_train_df['toxic'].sum()\n",
        "non_toxic_count = len(toxicity_train_df) - toxic_count\n",
        "\n",
        "# Plot side-by-side bars for toxic and non-toxic comments\n",
        "labels = ['Toxic Comments', 'Non-Toxic Comments']\n",
        "counts = [toxic_count, non_toxic_count]\n",
        "\n",
        "plt.bar(labels, counts, color=['red', 'blue'])\n",
        "plt.ylabel('Comment Count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQBDfIDTAVmi"
      },
      "source": [
        "# Visualization of toxicity in test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "vMgpw28TAYN6",
        "outputId": "d5314cb0-13c7-449f-c19d-c1c81f11ab59"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Count toxic and non-toxic comments\n",
        "toxic_count = toxicity_test_df['toxic'].sum()\n",
        "non_toxic_count = len(toxicity_test_df) - toxic_count\n",
        "\n",
        "# Plot side-by-side bars for toxic and non-toxic comments\n",
        "labels = ['Toxic Comments', 'Non-Toxic Comments']\n",
        "counts = [toxic_count, non_toxic_count]\n",
        "\n",
        "plt.bar(labels, counts, color=['red', 'blue'])\n",
        "plt.ylabel('Comment Count')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWP0mBB9ZFf4"
      },
      "source": [
        "# Splitting and Labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LUVKLSjZKHn",
        "outputId": "508f3b54-8973-4567-f5ef-7a159aafe796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_texts:\n",
            "[' didn\\'t he?\"', ' white supremacists and all other left wing groups.\"', ' this man deserves the chair. He ruined the lives of these girls and THEN tries to have them killed? He needs to die... preferably suffering.\"', ' and casts his votes based on fact based common sense. Common sense has been missing in Eugene for a very long time! We sincerely need Clark. Its time for common sense to come back to our community!\"', ' with a disabled plane or anything else outside of a lethal weapon. You\\'re exaggerating because you don\\'t like her or her paper.\"']\n",
            "train_labels:\n",
            "[0.0, 1.0, 1.0, 0.0, 1.0]\n",
            "test_texts\n",
            "['BS !', 'Hodad...does your user handle refer to your daughter??', \"Many of them were wearing red Na'i Aupuni shirts.  Does that not show support for federal recognition?\", '\"Methinks your comment is indicative of your need to understand the Christian faith which does not require the subservience of woman as much as care and love for them by the man - \"\"as Christ loved the church\"\".\"', \"And sometimes it's just salting the earth...\"]\n",
            "test_labels:\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# model_name = \"roberta-base\"\n",
        "\n",
        "# toxicity_train_df.reset_index(drop=True, inplace=True)\n",
        "# toxicity_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# X_train = toxicity_train_df[['comment_text']].reset_index(drop=True)\n",
        "# X_train = X_train.dropna()\n",
        "\n",
        "# # y_train = toxicity_train_df[['obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']]\n",
        "# y_train = toxicity_train_df[['toxic']].reset_index(drop=True)\n",
        "# y_train = y_train.dropna()\n",
        "# # toxicity_train_df.info()\n",
        "\n",
        "\n",
        "# # toxicity_test_df.info()\n",
        "# X_test = toxicity_test_df[['comment_text']].reset_index(drop=True)\n",
        "# X_test = X_test.dropna()\n",
        "# # # y_test = toxicity_test_df[['obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']]\n",
        "# y_test = toxicity_test_df[['toxic']].reset_index(drop=True)\n",
        "# y_test = y_test.dropna()\n",
        "\n",
        "\n",
        "# train_texts = X_train['comment_text'].tolist()\n",
        "# train_labels = y_train\n",
        "# test_texts = X_test['comment_text'].tolist()\n",
        "# test_labels = y_test\n",
        "\n",
        "# # See examples of texts & labels\n",
        "# print(\"train_texts:\")\n",
        "# print(train_texts[:5])\n",
        "# print(\"train_labels:\")\n",
        "# print(train_labels[:5])\n",
        "# print(\"test_texts\")\n",
        "# print(test_texts[:5])\n",
        "# print(\"test_labels:\")\n",
        "# print(test_labels[:5])\n",
        "\n",
        "# train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "# Reset index to ensure consistency\n",
        "toxicity_train_df.reset_index(drop=True, inplace=True)\n",
        "toxicity_test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Select relevant columns from DataFrame and drop NaN values\n",
        "train_data = toxicity_train_df[['comment_text', 'toxic']].dropna()\n",
        "test_data = toxicity_test_df[['comment_text', 'toxic']].dropna()\n",
        "\n",
        "# Extract features and labels\n",
        "train_texts = train_data['comment_text'].tolist()\n",
        "train_labels = train_data['toxic'].tolist()\n",
        "test_texts = test_data['comment_text'].tolist()\n",
        "test_labels = test_data['toxic'].tolist()\n",
        "\n",
        "# Print examples of texts & labels\n",
        "print(\"train_texts:\")\n",
        "print(train_texts[:5])\n",
        "print(\"train_labels:\")\n",
        "print(train_labels[:5])\n",
        "print(\"test_texts\")\n",
        "print(test_texts[:5])\n",
        "print(\"test_labels:\")\n",
        "print(test_labels[:5])\n",
        "\n",
        "# Split train data into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PaVNtTit2tc"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m54fSxNKt4oC",
        "outputId": "fbfee85a-e825-4225-a281-2da9dbfb46e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset\n",
            "Sample 1:\n",
            "Encoding keys: ['<s>', 'or', 'what', 'is', 'exclusively', 'liberal', 'about', 'good', 'food', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 2:\n",
            "Encoding keys: ['<s>', 'and', 'until', 'last', 'year', 'LC', 'FM', 'mandated', 'all', 'vendors', 'engage', 'in', 'illegal', 'price', 'fixing', '.', 'Any', 'vendor', 'that', 'honors', '\"\"', 'buy', 'direct', 'from', 'the', 'grow', 'er', 'and', 'save', '\"\"', 'is', 'ran', 'off', 'through', 'a', 'g', 'auntlet', 'of', 'criminal', 'behavior', 'our', 'county', 'board', 'and', 'city', 'council', 'has', 'protected', 'from', 'criminal', 'prosecution', '.\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 3:\n",
            "Encoding keys: ['<s>', 'and', 'pro', 'brain', 'use', '.\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Val Dataset\n",
            "Sample 1:\n",
            "Encoding keys: ['<s>', 'you', 'will', 'ben', 'if', 'it', 'greatly', 'from', 'Bernie', \"'s\", 'free', 'college', 'plan', '.\"', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 2:\n",
            "Encoding keys: ['<s>', 'under', 'the', 'table', 'labor', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 3:\n",
            "Encoding keys: ['<s>', 'the', 'fact', 'is', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Test Dataset\n",
            "Sample 1:\n",
            "Encoding keys: ['<s>', 'BS', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 2:\n",
            "Encoding keys: ['<s>', 'H', 'od', 'ad', '...', 'does', 'your', 'user', 'handle', 'refer', 'to', 'your', 'daughter', '??', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n",
            "Sample 3:\n",
            "Encoding keys: ['<s>', 'Many', 'of', 'them', 'were', 'wearing', 'red', 'Na', \"'\", 'i', 'A', 'up', 'uni', 'shirts', '.', '', 'Does', 'that', 'not', 'show', 'support', 'for', 'federal', 'recognition', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "Label: 0.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "\n",
        "class ToxicDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx]).float()\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = ToxicDataset(train_encodings, train_labels)\n",
        "val_dataset = ToxicDataset(val_encodings, val_labels)\n",
        "test_dataset = ToxicDataset(test_encodings, test_labels)\n",
        "\n",
        "print(\"Train Dataset\")\n",
        "# Iterate over train_dataset and print some samples\n",
        "for i in range(3):  # Print first 3 samples\n",
        "    sample = train_dataset[i]\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    # Convert input_ids tensor to list and access its keys\n",
        "    encoding_keys = tokenizer.convert_ids_to_tokens(sample[\"input_ids\"].tolist())\n",
        "    print(\"Encoding keys:\", encoding_keys)  # Print keys of encoding\n",
        "    print(\"Label:\", sample[\"labels\"].item())  # Print label\n",
        "    print()\n",
        "\n",
        "print(\"Val Dataset\")\n",
        "# Iterate over val dataset and print some samples\n",
        "for i in range(3):  # Print first 3 samples\n",
        "    sample = val_dataset[i]\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    # Convert input_ids tensor to list and access its keys\n",
        "    encoding_keys = tokenizer.convert_ids_to_tokens(sample[\"input_ids\"].tolist())\n",
        "    print(\"Encoding keys:\", encoding_keys)  # Print keys of encoding\n",
        "    print(\"Label:\", sample[\"labels\"].item())  # Print label\n",
        "    print()\n",
        "\n",
        "print(\"Test Dataset\")\n",
        "# Iterate over test dataset and print some samples\n",
        "for i in range(3):  # Print first 3 samples\n",
        "    sample = test_dataset[i]\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    # Convert input_ids tensor to list and access its keys\n",
        "    encoding_keys = tokenizer.convert_ids_to_tokens(sample[\"input_ids\"].tolist())\n",
        "    print(\"Encoding keys:\", encoding_keys)  # Print keys of encoding\n",
        "    print(\"Label:\", sample[\"labels\"].item())  # Print label\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4spOoBGGwLqb"
      },
      "source": [
        "# Tokenizer + Encodings + Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gLsm7774wWa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "29cd530a-3f54-4f2b-d235-e6e518a71aae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-9aa4fa20a7dc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.squeeze()\n",
        "\n",
        "    # Apply sigmoid activation function to convert logits to probabilities\n",
        "    preds = 1 / (1 + np.exp(-preds))\n",
        "\n",
        "    # Round probabilities to get binary predictions\n",
        "    binary_preds = np.round(preds)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, binary_preds)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, binary_preds, average='binary')\n",
        "\n",
        "    # Calculate AUC-ROC\n",
        "    auc_roc = roc_auc_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc_roc': auc_roc\n",
        "    }\n",
        "\n",
        "# Define the model with BCE loss\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
        "\n",
        "# Define the BCE loss function\n",
        "loss_function = \"bce_with_logits\"  # Binary Cross-Entropy Loss\n",
        "\n",
        "# Define the training arguments (GPU Version)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=128,\n",
        "    warmup_steps=200,\n",
        "    learning_rate=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=1000,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        ")\n",
        "\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "model.save_pretrained('/results/fine_tuned_roberta_model')\n",
        "\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "# encoded_input = tokenizer(text, return_tensors='pt')\n",
        "# output = model(**encoded_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcEulzfF5at2"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRuSNgIQ5fz1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Use the trained model for evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"Evaluation results on the validation dataset:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Make predictions on the validation set\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "# Extract predicted labels and logits\n",
        "predicted_labels = predictions.predictions.squeeze()\n",
        "predicted_probs = 1 / (1 + np.exp(-predicted_labels))\n",
        "\n",
        "# Step 1: Collect Uncalibrated Predictions\n",
        "uncalibrated_predictions = predicted_labels\n",
        "\n",
        "# Step 2: Fit Calibration Model (Isotonic Regression)\n",
        "calibration_model = IsotonicRegression(out_of_bounds='clip')\n",
        "calibrated_probabilities = calibration_model.fit_transform(uncalibrated_predictions, val_labels)\n",
        "\n",
        "# Use a threshold of 0.50 to classify texts as toxic (1) or non-toxic (0)\n",
        "threshold = 0.50\n",
        "binary_predictions = (calibrated_probabilities >= threshold).astype(float)\n",
        "\n",
        "# Compare predicted labels with actual labels\n",
        "for i in range(len(val_texts)):\n",
        "    print(f\"Text: {val_texts[i]}\")\n",
        "    print(f\"Predicted Probability: {calibrated_probabilities[i]}\")\n",
        "    print(f\"Predicted Label: {binary_predictions[i]}\")\n",
        "    print(f\"Actual Label: {val_labels[i]}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert predictions to probabilities and get class labels\n",
        "predicted_probabilities = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)\n",
        "predicted_labels = torch.argmax(predicted_probabilities, dim=1)\n",
        "\n",
        "# Flatten the true labels\n",
        "true_labels = np.array(val_labels)\n",
        "\n",
        "# Calculate accuracy and print classification report\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(true_labels, predicted_labels))\n",
        "\n",
        "# Count occurrences of each class in true labels\n",
        "true_label_counts = np.bincount(true_labels.astype(int))\n",
        "\n",
        "# Count occurrences of each class in predicted labels\n",
        "predicted_label_counts = np.bincount(binary_predictions.astype(int))\n",
        "\n",
        "# Print the counts\n",
        "print(\"True Label Counts (Class 0 and 1):\", true_label_counts)\n",
        "print(\"Predicted Label Counts (Class 0 and 1):\", predicted_label_counts)\n",
        "\n",
        "# Separate predicted probabilities based on actual labels\n",
        "toxic_probs = predicted_probs[np.where(np.array(val_labels) == 1)]\n",
        "non_toxic_probs = predicted_probs[np.where(np.array(val_labels) == 0)]\n",
        "\n",
        "# Plot histograms of predicted probabilities for both classes\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.hist(toxic_probs, bins=60, alpha=0.5, color='red', label='Toxic Comments')\n",
        "plt.hist(non_toxic_probs, bins=60, alpha=0.5, color='blue', label='Non-Toxic Comments')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities on Validation Set')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7-tPE46AzI"
      },
      "source": [
        "# Predictions on Twitch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA8lhxmU6C7T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model for inference\n",
        "tokenizer = RobertaTokenizer.from_pretrained('/content/results/checkpoint-3000/')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/content/results/checkpoint-3000/')\n",
        "\n",
        "twitch_df = pd.read_csv('twitch_toxicity.csv')\n",
        "print(twitch_df.head(6))\n",
        "print(twitch_df.info())\n",
        "\n",
        "# Create a text classification pipeline\n",
        "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example function to apply the classifier to each row in the DataFrame\n",
        "# count = 0\n",
        "\n",
        "# Function to convert LABEL_0 to 'no'\n",
        "def convert_label(prediction):\n",
        "    return 'no' if prediction[0]['label'] == 'LABEL_0' else 'yes'\n",
        "\n",
        "# Example function to apply the classifier to each row in the DataFrame\n",
        "def predict_label(row):\n",
        "    text_to_predict = row['comment_text']\n",
        "    prediction = classifier(text_to_predict)\n",
        "    return convert_label(prediction)\n",
        "\n",
        "# Apply the prediction function to each row in the DataFrame\n",
        "twitch_df['roberta_prediction'] = twitch_df.head(300).apply(predict_label, axis=1)\n",
        "# twitch_df['roberta_prediction'] = twitch_df.apply(predict_label, axis=1)\n",
        "\n",
        "# Display the DataFrame with predictions\n",
        "print(twitch_df.head(300)[['comment_text', 'roberta_prediction', 'toxic']])\n",
        "\n",
        "# Assuming 'LABEL_0' corresponds to 'no'\n",
        "prediction_counts = twitch_df.head(300)['roberta_prediction'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Count of 'no':\", prediction_counts.get('no', 0))\n",
        "print(\"Count of 'yes':\", prediction_counts.get('yes', 0))  # Adjust the label if needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Native PyTorch (instead of HF Trainer)"
      ],
      "metadata": {
        "id": "d9EcJcm1nJkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_train_epochs = 1\n",
        "for epoch in range(num_train_epochs):\n",
        "  total_loss = 0.0\n",
        "  for batch_idx, batch in enumerate(train_loader):\n",
        "      optim.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      if (batch_idx + 1) % 100 == 0:  # Print progress every 100 batches\n",
        "          print(f\"Epoch [{epoch + 1}/{num_train_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {total_loss / (batch_idx + 1):.4f}\")\n",
        "\n",
        "  print(f\"Epoch [{epoch + 1}/{num_train_epochs}], Average Loss: {total_loss / len(train_loader):.4f}\")\n",
        "#   for batch in train_loader:\n",
        "#     optim.zero_grad()\n",
        "#     input_ids = batch['input_ids'].to(device)\n",
        "#     attention_mask = batch['attention_mask'].to(device)\n",
        "#     labels = batch['labels'].to(device)\n",
        "\n",
        "#     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "#     loss = outputs[0]\n",
        "#     loss.backward()\n",
        "#     optim.step()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "model.save_pretrained('/usr/fine_tuned_roberta_model')\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/usr/fine_tuned_roberta_model')\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Assuming you have a validation DataLoader named val_loader\n",
        "num_toxic_correct = 0\n",
        "num_non_toxic_correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        predictions = torch.sigmoid(outputs.logits).squeeze()  # Assuming binary classification and using sigmoid activation\n",
        "\n",
        "        predicted_labels = (predictions >= 0.5).float()\n",
        "\n",
        "        num_toxic_correct += ((predicted_labels == 1) & (labels == 1)).sum().item()\n",
        "        num_non_toxic_correct += ((predicted_labels == 0) & (labels == 0)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy_toxic = num_toxic_correct / total\n",
        "accuracy_non_toxic = num_non_toxic_correct / total\n",
        "\n",
        "print(\"Accuracy for toxic comments:\", accuracy_toxic)\n",
        "print(\"Accuracy for non-toxic comments:\", accuracy_non_toxic)"
      ],
      "metadata": {
        "id": "Yqw8qYpfmtl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b1a13f-f790-4c86-a2e0-073278408f30"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Average Loss: 0.2442\n",
            "Accuracy for toxic comments: 0.3652173913043478\n",
            "Accuracy for non-toxic comments: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default RoBERTa test as baseline"
      ],
      "metadata": {
        "id": "60_Yekc0nJdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Set a fixed state for randomness\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # If using CUDA\n",
        "\n",
        "# Load the tokenizer and model for inference\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "# Move the model to CPU if it's on CUDA device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to convert LABEL_0 to 'no'\n",
        "def convert_label(prediction):\n",
        "    return 'no' if prediction[0]['label'] == 'LABEL_0' else 'yes'\n",
        "\n",
        "global count\n",
        "count = 0\n",
        "\n",
        "def predict_label(row):\n",
        "    global count\n",
        "    text_to_predict = row['comment_text']\n",
        "    if(count <100):\n",
        "      print(text_to_predict)\n",
        "    encoding = tokenizer(text_to_predict, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "    if(count <100):\n",
        "      print(probabilities)\n",
        "    # Convert probabilities to binary labels\n",
        "    binary_label = 1 if ((np.abs(probabilities[1] - probabilities[0]) <= 0.035) and (probabilities[1] > 0.5275) and (probabilities[0] < 0.494)) else 0\n",
        "    count = count + 1\n",
        "    if (count % 100 == 0):\n",
        "      print(count)\n",
        "    if(count <100):\n",
        "      print(np.abs(probabilities[1] - probabilities[0]))\n",
        "      print(binary_label)\n",
        "    # if (binary_label == 1):\n",
        "    #   print(text_to_predict)\n",
        "    return binary_label\n",
        "\n",
        "# Load the Twitch dataset\n",
        "twitch_df = pd.read_csv('twitch_toxicity.csv')\n",
        "print(len(twitch_df))\n",
        "\n",
        "# Apply the prediction function to each row in the DataFrame\n",
        "twitch_df['roberta_prediction'] = twitch_df.head(200).apply(predict_label, axis=1)\n",
        "\n",
        "# Display the DataFrame with predictions\n",
        "print(twitch_df[['comment_text', 'roberta_prediction']])\n",
        "\n",
        "# Assuming 'LABEL_0' corresponds to 'no'\n",
        "prediction_counts = twitch_df.head(200)['roberta_prediction'].value_counts()\n",
        "print(prediction_counts)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Count of 'no':\", prediction_counts[0.0])\n",
        "print(\"Count of 'yes':\", prediction_counts[1.0])  # Adjust the label if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "BfSPaPT-zglZ",
        "outputId": "3cf5198b-0951-437c-cca9-1442eb4c4efe"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-44-9bbe899acbd4>, line 46)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-9bbe899acbd4>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    binary_label = 1 if (np.abs(probabilities[0] > 0.55) else 0\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuned Roberta on Twitch Dataset"
      ],
      "metadata": {
        "id": "CgA8eGog8O1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# Set a fixed state for randomness\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)  # If using CUDA\n",
        "\n",
        "# Load the tokenizer and model for inference\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/usr/fine_tuned_roberta_model')\n",
        "\n",
        "# Move the model to CPU if it's on CUDA device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to convert LABEL_0 to 'no'\n",
        "def convert_label(prediction):\n",
        "    return 'no' if prediction[0]['label'] == 'LABEL_0' else 'yes'\n",
        "\n",
        "global count\n",
        "global sub_mention\n",
        "sub_mention = False\n",
        "count = 0\n",
        "\n",
        "def predict_label(row):\n",
        "    global count\n",
        "    global sub_mention\n",
        "    text_to_predict = row['comment_text']\n",
        "    if (\"Tier 1\") in text_to_predict:\n",
        "      sub_mention = True\n",
        "    else:\n",
        "      sub_mention = False\n",
        "    # if(count <100):\n",
        "    #   print(text_to_predict)\n",
        "    encoding = tokenizer(text_to_predict, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "    # if(count <100):\n",
        "    #   print(probabilities)\n",
        "    # Convert probabilities to binary labels\n",
        "    binary_label = 1 if (np.abs(probabilities[0] > 0.592745) and not sub_mention) else 0\n",
        "    count = count + 1\n",
        "    # if (count % 100 == 0):\n",
        "    #   print(count)\n",
        "    # if(count <100):\n",
        "    #   print(np.abs(probabilities[0]))\n",
        "    #   print(binary_label)\n",
        "    # if (binary_label == 1):\n",
        "    #   print(text_to_predict)\n",
        "    print(binary_label)\n",
        "    return binary_label\n",
        "\n",
        "# Load the Twitch dataset\n",
        "twitch_df = pd.read_csv('twitch_toxicity.csv')\n",
        "print(len(twitch_df))\n",
        "\n",
        "# Apply the prediction function to each row in the DataFrame\n",
        "twitch_df['roberta_prediction'] = twitch_df.head(500).apply(predict_label, axis=1)\n",
        "\n",
        "# Display the DataFrame with predictions\n",
        "print(twitch_df[['comment_text', 'roberta_prediction']])\n",
        "\n",
        "# Assuming 'LABEL_0' corresponds to 'no'\n",
        "prediction_counts = twitch_df.head(500)['roberta_prediction'].value_counts()\n",
        "print(prediction_counts)\n",
        "\n",
        "# Print the counts\n",
        "print(\"Count of 'no':\", prediction_counts[0.0])\n",
        "print(\"Count of 'yes':\", prediction_counts[1.0])  # Adjust the label if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rodAVSjc8RRu",
        "outputId": "75ac758a-58ea-494a-8a9d-f332a11d699c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11449\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "                                            comment_text  roberta_prediction\n",
            "0      Confused why people are spamming things like J...                 0.0\n",
            "1           Nice thats my first time in Hasan Lifestream                 0.0\n",
            "2         THE ONE PIECE IS REAL LETSGO NEWS TODAY LETSGO                 0.0\n",
            "3      Dont forget to check ur cocks before stream st...                 1.0\n",
            "4      THE HASANABI IS REAL LETSGO THE HASANABI IS RE...                 0.0\n",
            "...                                                  ...                 ...\n",
            "11444                You guys don't need to be mean, lol                 NaN\n",
            "11445              I joined at the wrong time dang Sadge                 NaN\n",
            "11446  hasL you did great azan fuck chat - thank you ...                 NaN\n",
            "11447              why does he get so feelings from valo                 NaN\n",
            "11448        @boqm5 Valkyrae - Sykkuno - Miyoung - Wendy                 NaN\n",
            "\n",
            "[11449 rows x 2 columns]\n",
            "0.0    348\n",
            "1.0    152\n",
            "Name: roberta_prediction, dtype: int64\n",
            "Count of 'no': 348\n",
            "Count of 'yes': 152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "space_delimited_data = \"\"\"\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "1\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "0\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "0\n",
        "1\n",
        "\"\"\"\n",
        "\n",
        "# Split the data by space and create a list of numbers\n",
        "numbers = space_delimited_data.split()\n",
        "\n",
        "# Create a DataFrame with a single column\n",
        "df = pd.DataFrame({'Numbers': numbers})\n",
        "\n",
        "# Export the DataFrame to Excel\n",
        "df.to_excel('/usr/robertafivehundo.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ZeHEylzbBOr8"
      },
      "execution_count": 59,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}