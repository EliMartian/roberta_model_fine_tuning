# -*- coding: utf-8 -*-
"""fine_tuning_roberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XqGKKEsIt1h3uKABPoHVDIbcsl0KTI9R

# Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip show transformers
# !pip show accelerate
# !pip install transformers[torch] -U
# !pip install accelerate -U
# !pip install transformers
# !pip install pytorch-lightning
# !pip install --upgrade transformers

"""# Preprocessing"""

import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification


# 1. Prepare Dataset
# 2. Load pretrained Tokenizer, call it with dataset -> encoding
# 3. Build PyTorth Dataset with encodings
# 4. Load pretrained Model
# 5. Load HF Trainer and train it


# Train Data - 52662 rows after pre-processing (done above)
toxicity_train_df = pd.read_csv('toxicity_train.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)
toxicity_train_df = toxicity_train_df.dropna()


# Test Data - 7287 rows after pre-processing
toxicity_test_df = pd.read_csv('toxicity_test.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONE)
toxicity_test_df = toxicity_test_df.dropna()


# Sampling for CPU use
# fraction = 0.1

# toxicity_train_df = toxicity_train_df.sample(frac=fraction, random_state=42)
# toxicity_test_df = toxicity_test_df.sample(frac=fraction, random_state=42)

# # Reset index for consistency
# toxicity_train_df.reset_index(drop=True, inplace=True)
# toxicity_test_df.reset_index(drop=True, inplace=True)

# List of categories to check
categories_to_check = ['obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']

# Check if any category is above the 0.5 threshold
toxicity_train_df[categories_to_check] = toxicity_train_df[categories_to_check].apply(pd.to_numeric, errors='coerce')
toxicity_test_df[categories_to_check] = toxicity_test_df[categories_to_check].apply(pd.to_numeric, errors='coerce')

# Check if any category is above the 0.5 threshold
toxicity_train_df['toxic'] = (toxicity_train_df[categories_to_check] >= 0.5).any(axis=1).astype(float)
toxicity_test_df['toxic'] = (toxicity_test_df[categories_to_check] >= 0.5).any(axis=1).astype(float)

# Convert boolean values to 1.0 for True and 0.0 for False
toxicity_train_df['toxic'] = toxicity_train_df['toxic'].astype(float)
toxicity_test_df['toxic'] = toxicity_test_df['toxic'].astype(float)

toxicity_train_df = toxicity_train_df[['comment_text', 'toxic', 'obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']]
toxicity_test_df = toxicity_test_df[['comment_text', 'toxic', 'obscene', 'sexual_explicit', 'threat', 'insult', 'identity_attack']]


# Fraction of the majority class you want to keep (e.g., 0.2 for 20%)
undersample_fraction = 0.33

# Separate the majority and minority classes in training data
majority_class_train = toxicity_train_df[toxicity_train_df['toxic'] == 0]
minority_class_train = toxicity_train_df[toxicity_train_df['toxic'] == 1]

# Undersample the majority class in training data
undersampled_majority_class_train = majority_class_train.sample(frac=undersample_fraction, random_state=42)

# Concatenate the undersampled majority class with the minority class in training data
undersampled_train_df = pd.concat([undersampled_majority_class_train, minority_class_train])

# Shuffle the undersampled training dataframe
toxicity_train_df = undersampled_train_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Separate the majority and minority classes in test data
majority_class_test = toxicity_test_df[toxicity_test_df['toxic'] == 0]
minority_class_test = toxicity_test_df[toxicity_test_df['toxic'] == 1]

# Display the class distribution before and after undersampling
print("\nClass distribution in training data after undersampling:")
print(toxicity_train_df['toxic'].value_counts())

print("\nClass distribution in test data before undersampling:")
print(toxicity_test_df['toxic'].value_counts())

"""Test Lengths of DFs"""

print(len(toxicity_train_df))
print(len(toxicity_test_df))

"""# Visualization of toxicity in train"""

# Count toxic and non-toxic comments
toxic_count = toxicity_train_df['toxic'].sum()
non_toxic_count = len(toxicity_train_df) - toxic_count

# Plot side-by-side bars for toxic and non-toxic comments
labels = ['Toxic Comments', 'Non-Toxic Comments']
counts = [toxic_count, non_toxic_count]

plt.bar(labels, counts, color=['red', 'blue'])
plt.ylabel('Comment Count')

plt.show()

"""# Visualization of toxicity in test"""

# Count toxic and non-toxic comments
toxic_count = toxicity_test_df['toxic'].sum()
non_toxic_count = len(toxicity_test_df) - toxic_count

# Plot side-by-side bars for toxic and non-toxic comments
labels = ['Toxic Comments', 'Non-Toxic Comments']
counts = [toxic_count, non_toxic_count]

plt.bar(labels, counts, color=['red', 'blue'])
plt.ylabel('Comment Count')

plt.show()

"""# Splitting and Labelling"""

model_name = "roberta-base"

toxicity_train_df.reset_index(drop=True, inplace=True)
toxicity_test_df.reset_index(drop=True, inplace=True)

X_train = toxicity_train_df[['comment_text']].reset_index(drop=True)
X_train = X_train.dropna()

# y_train = toxicity_train_df[['obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']]
y_train = toxicity_train_df[['toxic']].reset_index(drop=True)
y_train = y_train.dropna()
# toxicity_train_df.info()


# toxicity_test_df.info()
X_test = toxicity_test_df[['comment_text']].reset_index(drop=True)
X_test = X_test.dropna()
# # y_test = toxicity_test_df[['obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']]
y_test = toxicity_test_df[['toxic']].reset_index(drop=True)
y_test = y_test.dropna()


train_texts = X_train['comment_text'].tolist()
train_labels = y_train
test_texts = X_test['comment_text'].tolist()
test_labels = y_test

# See examples of texts & labels
print("train_texts:")
print(train_texts[:5])
print("train_labels:")
print(train_labels[:5])
print("test_texts")
print(test_texts[:5])
print("test_labels:")
print(test_labels[:5])

train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)

"""# Dataset"""

from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel


class ToxicDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx]).float()
        return item

    def __len__(self):
        return len(self.labels)

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

train_labels = train_labels['toxic'].reset_index(drop=True)
val_labels = val_labels['toxic'].reset_index(drop=True)
test_labels = test_labels['toxic'].reset_index(drop=True)

# Convert Pandas Series to lists
train_labels_list = train_labels.tolist()
val_labels_list = val_labels.tolist()
test_labels_list = test_labels.tolist()

train_dataset = ToxicDataset(train_encodings, train_labels)
val_dataset = ToxicDataset(val_encodings, val_labels)
test_dataset = ToxicDataset(test_encodings, test_labels)

"""# Tokenizer + Encodings + Training"""

from transformers import EarlyStoppingCallback, Trainer, TrainingArguments

# training_args = TrainingArguments(
#     output_dir='./results',
#     num_train_epochs=1,
#     per_device_train_batch_size=4,
#     per_device_eval_batch_size=16,
#     warmup_steps=100,
#     learning_rate=1e-4,
#     weight_decay=0.01,
#     logging_dir='./logs',
#     logging_steps=5000,
#     evaluation_strategy="steps",
#     eval_steps=5000,
#     save_strategy="steps",
#     load_best_model_at_end=True,
#     metric_for_best_model="eval_loss",
# )

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=100,
    learning_rate=1e-5,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=1000,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=1000,  # Set save_steps to the same value as logging_steps
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)

model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=1)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.02)],
)
trainer.train()

model.save_pretrained('/results/fine_tuned_roberta_model')

# text = "Replace me by any text you'd like."
# encoded_input = tokenizer(text, return_tensors='pt')
# output = model(**encoded_input)

"""# Evaluation"""

from sklearn.metrics import accuracy_score, classification_report

# Use the trained model for evaluation
eval_results = trainer.evaluate()

# Print evaluation results
print(eval_results)

# Make predictions on the validation set
predictions = trainer.predict(val_dataset)

# Convert predictions to probabilities and get class labels
predicted_probabilities = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)
predicted_labels = torch.argmax(predicted_probabilities, dim=1)

# Flatten the true labels
true_labels = val_labels.values.flatten()

# Calculate accuracy and print classification report
accuracy = accuracy_score(true_labels, predicted_labels)
print(f"Accuracy: {accuracy}")
print(classification_report(true_labels, predicted_labels))



# Count occurrences of each class in true labels
true_label_counts = np.bincount(true_labels.astype(int))

# Count occurrences of each class in predicted labels
predicted_label_counts = np.bincount(predicted_labels.numpy().astype(int))

# Print the counts
print("True Label Counts (Class 0 and 1):", true_label_counts)
print("Predicted Label Counts (Class 0 and 1):", predicted_label_counts)

"""# Predictions on Twitch Dataset"""

import pandas as pd
from transformers import RobertaTokenizer, AutoModelForSequenceClassification
from transformers import pipeline

# Load the fine-tuned model for inference
tokenizer = RobertaTokenizer.from_pretrained('/content/results/checkpoint-2000/')
model = AutoModelForSequenceClassification.from_pretrained('/content/results/checkpoint-2000/')

twitch_df = pd.read_csv('twitch_toxicity.csv')
print(twitch_df.head(6))
print(twitch_df.info())

# Create a text classification pipeline
classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)

# Example function to apply the classifier to each row in the DataFrame
# count = 0

# Function to convert LABEL_0 to 'no'
def convert_label(prediction):
    return 'no' if prediction[0]['label'] == 'LABEL_0' else 'yes'

# Example function to apply the classifier to each row in the DataFrame
def predict_label(row):
    text_to_predict = row['comment_text']
    prediction = classifier(text_to_predict)
    return convert_label(prediction)

# Apply the prediction function to each row in the DataFrame
twitch_df['roberta_prediction'] = twitch_df.head(300).apply(predict_label, axis=1)
# twitch_df['roberta_prediction'] = twitch_df.apply(predict_label, axis=1)

# Display the DataFrame with predictions
print(twitch_df.head(300)[['comment_text', 'roberta_prediction', 'toxic']])

# Assuming 'LABEL_0' corresponds to 'no'
prediction_counts = twitch_df.head(300)['roberta_prediction'].value_counts()

# Print the counts
print("Count of 'no':", prediction_counts.get('no', 0))
print("Count of 'yes':", prediction_counts.get('yes', 0))  # Adjust the label if needed

